{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold;\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Exploration, Engineering and Cleaning \n",
    "\n",
    "Now we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities and one hot encode any categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Store our passenger ID for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'PassengerId', u'Survived', u'Pclass', u'Name', u'Sex', u'Age',\n",
       "       u'SibSp', u'Parch', u'Ticket', u'Fare', u'Cabin', u'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the train and test datasets\n",
    "full_data = [train, test]\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age / Cabin / Embarked have empty values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pclass ##\n",
    "there is no missing value on this feature and already a numerical value. so let's check it's impact on our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n"
     ]
    }
   ],
   "source": [
    "print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sex ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n"
     ]
    }
   ],
   "source": [
    "print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SibSp and Parch ##\n",
    "With the number of siblings/spouse and the number of children/parents we can create new feature called Family Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate new feature - Is Alone ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embarked ##\n",
    "the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fare ##\n",
    "Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CategoricalFare  Survived\n",
      "0   (-0.001, 7.91]  0.197309\n",
      "1   (7.91, 14.454]  0.303571\n",
      "2   (14.454, 31.0]  0.454955\n",
      "3  (31.0, 512.329]  0.581081\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Age ##\n",
    "we have plenty of missing values in this feature. # generate random numbers between (mean - std) and (mean + std).\n",
    "then we categorize age into 5 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CategoricalAge  Survived\n",
      "0  (-0.08, 16.0]  0.513274\n",
      "1   (16.0, 32.0]  0.360092\n",
      "2   (32.0, 48.0]  0.366412\n",
      "3   (48.0, 64.0]  0.434783\n",
      "4   (64.0, 80.0]  0.090909\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg \t   = dataset['Age'].mean()\n",
    "    age_std \t   = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    \n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "\n",
    "print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Name ##\n",
    "inside this feature we can find the title of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex       female  male\n",
      "Title                 \n",
      "Capt           0     1\n",
      "Col            0     2\n",
      "Countess       1     0\n",
      "Don            0     1\n",
      "Dr             1     6\n",
      "Jonkheer       0     1\n",
      "Lady           1     0\n",
      "Major          0     2\n",
      "Master         0    40\n",
      "Miss         182     0\n",
      "Mlle           2     0\n",
      "Mme            1     0\n",
      "Mr             0   517\n",
      "Mrs          125     0\n",
      "Ms             1     0\n",
      "Rev            0     6\n",
      "Sir            0     1\n"
     ]
    }
   ],
   "source": [
    "def get_title(name):\n",
    "\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "\t# If the title exists, extract and return it.\n",
    "\tif title_search:\n",
    "\t\treturn title_search.group(1)\n",
    "\treturn \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "print(pd.crosstab(train['Title'], train['Sex']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " so we have titles. let's categorize it and check the title impact on survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning #\n",
    "great! now let's clean our data and map our features into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex  Age     Fare  Embarked  FamilySize  IsAlone  Title\n",
      "0         0       3    1   22   7.2500         0           2        0      1\n",
      "1         1       1    0   38  71.2833         1           2        0      3\n",
      "2         1       3    0   26   7.9250         0           1        1      2\n",
      "3         1       1    0   35  53.1000         0           2        0      3\n",
      "4         0       3    1   35   8.0500         0           1        1      1\n",
      "5         0       3    1   33   8.4583         2           1        1      1\n",
      "6         0       1    1   54  51.8625         0           1        1      1\n",
      "7         0       3    1    2  21.0750         0           5        0      4\n",
      "8         1       3    0   27  11.1333         0           3        0      3\n",
      "9         1       2    0   14  30.0708         1           2        0      3\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "#     dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "#     dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "#     dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "#     dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "#     dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "#     dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "#     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "#     dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "#     dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "#     dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n",
    "\n",
    "# Feature Selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n",
    "                 'Parch']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "\n",
    "print (train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train['Survived']\n",
    "train = train.drop(['Survived'], axis=1)\n",
    "x_train = train.values\n",
    "x_test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [u'Pclass', u'Title', u'Embarked', u'FamilySize']\n",
    "train_objs_num = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "dataset_preprocessed = pd.get_dummies(dataset, prefix=cat_columns, columns=cat_columns)\n",
    "x_train = dataset_preprocessed[:train_objs_num]\n",
    "x_test = dataset_preprocessed[train_objs_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a Machine Learning Model - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, params_needed=True):\n",
    "        if params_needed:\n",
    "            params['random_state'] = seed\n",
    "            self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test, scaling = True):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    if scaling:\n",
    "        scaler = StandardScaler()\n",
    "        columns = [u'Fare', u'Age']\n",
    "        x_train[columns] = scaler.fit_transform(x_train[columns])\n",
    "        x_test[columns] = scaler.transform(x_test[columns])\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train.as_matrix()[train_index]\n",
    "        y_tr = y_train.as_matrix()[train_index]\n",
    "        x_te = x_train.as_matrix()[test_index]\n",
    "        y_te = y_train.as_matrix()[test_index]\n",
    "\n",
    "        \n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        predictions = clf.predict(x_te)\n",
    "        score = accuracy_score(y_te, predictions)\n",
    "        print(score)\n",
    "        total_accuracy += score\n",
    "        oof_train[test_index] = predictions\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), total_accuracy / NFOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Accuracy of RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810055865922\n",
      "0.786516853933\n",
      "0.814606741573\n",
      "0.775280898876\n",
      "0.786516853933\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.7945954428472789)\n"
     ]
    }
   ],
   "source": [
    "rf_oof_train, rf_oof_test, total_accuracy = get_oof(rf,x_train, y_train, x_test)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing accuracy of ADA Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787709497207\n",
      "0.808988764045\n",
      "0.808988764045\n",
      "0.797752808989\n",
      "0.848314606742\n",
      "('Adaboost classifier - Total Accuracy with data cleanup and without one hot encoding', 0.8103508882053857)\n"
     ]
    }
   ],
   "source": [
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params={})\n",
    "ada_oof_train, ada_oof_test, total_accuracy = get_oof(ada,x_train, y_train, x_test)\n",
    "print(\"Adaboost classifier - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try one-hot encoding the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_with_pca(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    total_accuracy = 0.0\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    columns = [u'Fare', u'Age']\n",
    "    x_train[columns] = scaler.fit_transform(x_train[columns])\n",
    "    x_test[columns] = scaler.transform(x_test[columns])\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train.as_matrix()[train_index]\n",
    "        y_tr = y_train.as_matrix()[train_index]\n",
    "        x_te = x_train.as_matrix()[test_index]\n",
    "        y_te = y_train.as_matrix()[test_index]\n",
    "        \n",
    "        # PCA\n",
    "        pca = PCA(n_components=20, random_state=420)\n",
    "        pca2_results_train = pca.fit_transform(x_tr)\n",
    "        pca2_results_valid = pca.transform(x_te)\n",
    "        pca2_results_test = pca.transform(x_test)\n",
    "        \n",
    "        x_pca_tr = np.concatenate([pca2_results_train, x_tr], axis=1)\n",
    "        x_pca_te = np.concatenate([pca2_results_valid, x_te], axis=1)\n",
    "        x_pca_test = np.concatenate([pca2_results_test, x_test], axis=1)\n",
    "\n",
    "        clf.train(x_pca_tr, y_tr)\n",
    "\n",
    "        predictions = clf.predict(x_pca_te)\n",
    "        score = accuracy_score(y_te, predictions)\n",
    "        print(score)\n",
    "        total_accuracy += score\n",
    "        oof_train[test_index] = predictions\n",
    "        oof_test_skf[i, :] = clf.predict(x_pca_test)\n",
    "\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), total_accuracy / NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776536312849\n",
      "0.825842696629\n",
      "0.808988764045\n",
      "0.797752808989\n",
      "0.870786516854\n",
      "('Adaboost classifier - Total Accuracy with data cleanup and without one hot encoding', 0.81598141987320327)\n"
     ]
    }
   ],
   "source": [
    "ada_oof_train, ada_oof_test, total_accuracy = get_oof_with_pca(ada,x_train, y_train, x_test)\n",
    "print(\"Adaboost classifier - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77094972067\n",
      "0.780898876404\n",
      "0.775280898876\n",
      "0.775280898876\n",
      "0.820224719101\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.78452702278576358)\n"
     ]
    }
   ],
   "source": [
    "rf_oof_train, rf_oof_test, total_accuracy = get_oof_with_pca(rf,x_train, y_train, x_test)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 800, 'max_features': 'log2', 'max_depth': 8, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - Estimators\n",
    "rf_classifer_optimal =  RandomForestClassifier(n_jobs=-1)\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Use a grid over parameters of interest\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [120, 300, 500, 800, 1200],\n",
    "           \"max_depth\" : [6,8,15,25,30],\n",
    "           \"min_samples_leaf\" : [1, 2, 5,10],\n",
    "           \"max_features\" : ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "CV_rfc = RandomizedSearchCV(estimator=rf_classifer_optimal, param_distributions=param_grid, cv= 3)\n",
    "CV_rfc.fit(x_train, y_train)\n",
    "\n",
    "print (CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815642458101\n",
      "0.808988764045\n",
      "0.831460674157\n",
      "0.792134831461\n",
      "0.870786516854\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.82380264892348265)\n"
     ]
    }
   ],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=CV_rfc.best_params_)\n",
    "rf_oof_train, rf_oof_test, total_accuracy = get_oof_with_pca(rf,x_train, y_train, x_test)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776536312849\n",
      "0.825842696629\n",
      "0.808988764045\n",
      "0.797752808989\n",
      "0.870786516854\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.81598141987320327)\n"
     ]
    }
   ],
   "source": [
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params={})\n",
    "ada_oof_train, ada_oof_test, total_accuracy = get_oof_with_pca(ada, x_train, y_train, x_test)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821229050279\n",
      "0.803370786517\n",
      "0.808988764045\n",
      "0.780898876404\n",
      "0.870786516854\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.81705479881991094)\n"
     ]
    }
   ],
   "source": [
    "gbm = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params={})\n",
    "gbm_oof_train, gbm_oof_test, total_accuracy = get_oof_with_pca(gbm, x_train, y_train, x_test)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854748603352\n",
      "0.831460674157\n",
      "0.814606741573\n",
      "0.775280898876\n",
      "0.876404494382\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.830500282468144)\n"
     ]
    }
   ],
   "source": [
    "svc = SklearnHelper(clf=SVC, seed=SEED, params={})\n",
    "svc_oof_train, svc_oof_test, total_accuracy = get_oof_with_pca(svc, x_train, y_train, x_test)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810055865922\n",
      "0.820224719101\n",
      "0.825842696629\n",
      "0.780898876404\n",
      "0.848314606742\n",
      "('Overall Accuracy', 0.8170673529596385)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = SklearnHelper(clf=MLPClassifier, seed=SEED, params={})\n",
    "mlp_oof_train, mlp_oof_test, overall_accuracy = get_oof_with_pca(mlp, x_train, y_train, x_test)\n",
    "print(\"Overall Accuracy\", overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": "Viridis",
         "reversescale": true,
         "showscale": true,
         "type": "heatmap",
         "x": [
          "GBM",
          "MLP",
          "RF",
          "SVC"
         ],
         "y": [
          "GBM",
          "MLP",
          "RF",
          "SVC"
         ],
         "z": [
          [
           1,
           0.8597048830107465,
           0.9048168256662906,
           0.8019967478292973
          ],
          [
           0.8597048830107465,
           1,
           0.8887128905061356,
           0.8262382623978636
          ],
          [
           0.9048168256662906,
           0.8887128905061356,
           1,
           0.8161646645055379
          ],
          [
           0.8019967478292973,
           0.8262382623978636,
           0.8161646645055379,
           1
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"0b3f3c0d-6c1c-411b-8e94-3d9f5a62ab47\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0b3f3c0d-6c1c-411b-8e94-3d9f5a62ab47\", [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": true, \"y\": [\"GBM\", \"MLP\", \"RF\", \"SVC\"], \"x\": [\"GBM\", \"MLP\", \"RF\", \"SVC\"], \"z\": [[1.0, 0.8597048830107465, 0.9048168256662906, 0.8019967478292973], [0.8597048830107465, 1.0, 0.8887128905061356, 0.8262382623978636], [0.9048168256662906, 0.8887128905061356, 1.0, 0.8161646645055379], [0.8019967478292973, 0.8262382623978636, 0.8161646645055379, 1.0]], \"type\": \"heatmap\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"0b3f3c0d-6c1c-411b-8e94-3d9f5a62ab47\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0b3f3c0d-6c1c-411b-8e94-3d9f5a62ab47\", [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": true, \"y\": [\"GBM\", \"MLP\", \"RF\", \"SVC\"], \"x\": [\"GBM\", \"MLP\", \"RF\", \"SVC\"], \"z\": [[1.0, 0.8597048830107465, 0.9048168256662906, 0.8019967478292973], [0.8597048830107465, 1.0, 0.8887128905061356, 0.8262382623978636], [0.9048168256662906, 0.8887128905061356, 1.0, 0.8161646645055379], [0.8019967478292973, 0.8262382623978636, 0.8161646645055379, 1.0]], \"type\": \"heatmap\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_corr = pd.DataFrame( {\n",
    "     'SVC': svc_oof_train.ravel(), \n",
    "     'GBM' : gbm_oof_train.ravel(),\n",
    "     'RF' : rf_oof_train.ravel(),\n",
    "     'MLP' : mlp_oof_train.ravel()\n",
    "})\n",
    "x_test_corr = pd.DataFrame( {\n",
    "     'SVC': svc_oof_test.ravel(),\n",
    "     'GBM' : gbm_oof_test.ravel(),\n",
    "     'RF' : rf_oof_test.ravel(),\n",
    "     'MLP' : mlp_oof_test.ravel()\n",
    "})\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= x_train_corr.astype(float).corr().values ,\n",
    "        x=x_train_corr.columns.values,\n",
    "        y= x_train_corr.columns.values,\n",
    "          colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "py.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 120, 'max_features': 'sqrt', 'max_depth': 25, 'min_samples_leaf': 5}\n",
      "0.832402234637\n",
      "0.842696629213\n",
      "0.808988764045\n",
      "0.780898876404\n",
      "0.876404494382\n",
      "('Random Forest - Total Accuracy with data cleanup and without one hot encoding', 0.82827819973636296)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - Estimators\n",
    "rf_classifer_optimal =  RandomForestClassifier(n_jobs=-1)\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Use a grid over parameters of interest\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [120, 300, 500, 800, 1200],\n",
    "           \"max_depth\" : [6,8,15,25,30],\n",
    "           \"min_samples_leaf\" : [1, 2, 5,10],\n",
    "           \"max_features\" : ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "CV_rfc = RandomizedSearchCV(estimator=rf_classifer_optimal, param_distributions=param_grid, cv= 3)\n",
    "CV_rfc.fit(x_train_corr, y_train)\n",
    "\n",
    "print (CV_rfc.best_params_)\n",
    "\n",
    "rf_lev2 = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=CV_rfc.best_params_)\n",
    "rf_lev2_oof_train, rf_lev2_oof_test, total_accuracy = get_oof(rf,x_train_corr, y_train, x_test_corr, False)\n",
    "print(\"Random Forest - Total Accuracy with data cleanup and without one hot encoding\", total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
